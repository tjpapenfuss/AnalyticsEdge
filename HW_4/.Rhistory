((35000*2741)-(35000*1796)-((35000)*(rate)*945))
#Below is how you get the rate in which you break even.
((35000*2741)-(35000*1796)-((35000)*(rate)*945))/(4056+945)
for( i in seq(0, 3000, by = 100)){
newRate = (35000*1796)+((i)*4056)+(35000*(1-rate)*945)+((i)*945)
rate_value = c(rate_value, newRate)
rates = c(rates, i)
totalcost = c(totalcost, totalCostNoHelp)
newRate
}
#This is the rate at which you break even.
rate = 0.25
rate_value = c()
totalcost = c()
rates = c()
i = 0
for( i in seq(0, 3000, by = 100)){
newRate = (35000*1796)+((i)*4056)+(35000*(1-rate)*945)+((i)*945)
rate_value = c(rate_value, newRate)
rates = c(rates, i)
totalcost = c(totalcost, totalCostNoHelp)
newRate
}
# rates
ggplot(data.frame(rates, rate_value), aes(x = rates, y = rate_value)) +
ggtitle("Rate vs. Cost") + xlab("Rate") + ylab("Cost") + geom_line()+
geom_line(aes(x = rates, y = totalcost), col="red")
rate_value
#Below is how you get the rate in which you break even.
((35000*2741)-(35000*1796)-((35000)*(1-rate)*945))/(4056+945)
# rates
ggplot(data.frame(rates, rate_value), aes(x = rates, y = rate_value)) +
ggtitle("Cost of intervention vs. Total Cost") + xlab("Cost of Intervention") + ylab("Total Cost") + geom_line()+
geom_line(aes(x = rates, y = totalcost), col="red")
totalCostNoHelp = 35000*2741
#Below is how you get the rate in which you break even.
((35000*2741)-(35000*1796)-((35000)*(1-rate)*945))/(4056+945)
#This is the rate at which you break even.
rate = 0.25
rate_value = c()
totalcost = c()
rates = c()
i = 0
for( i in seq(0, 3000, by = 100)){
newRate = (35000*1796)+((i)*4056)+(35000*(1-rate)*945)+((i)*945)
rate_value = c(rate_value, newRate)
rates = c(rates, i)
totalcost = c(totalcost, totalCostNoHelp)
newRate
}
##### Problem 2: Housing Prices in Ames, Iowa, Revisited
### This problem should be realtively straightforward, and serves mostly as a review.
ames = read.csv("ames.csv")
set.seed(15071)
split = createDataPartition(ames$SalePrice, p = 0.7, list = FALSE) ames.train = ames[split,]
split = createDataPartition(ames$SalePrice, p = 0.7, list = FALSE)
ames.train = ames[split,]
ames.test = ames[-split,]
head(ames.train)
regressor = lm(SalePrice ~ ., data = ames.train)
summary(regressor)
head(ames.train)
cv.trees = train(y = SalePrice, x = ., method = "rpart",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
cv.trees = train(SalePrice~., method = "rpart",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
cv.trees = train(SalePrice ~ ., method = "rpart",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
cv.trees = train(SalePrice ~ ., method = "rpart", data = ames.train,
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
pdf('CART.pdf',12,12)
prp(cv.trees$finalModel,varlen=0,faclen=0,digits=3)
dev.off()
best.tree <- cv.trees$finalModel
cart.train.pred = predict(best.tree, newdata = ames.test)
best.tree
cart.train.pred = predict(best.tree, newdata = ames.test)
ames.test = ames[-split,]
best.tree <- cv.trees$finalModel
cart.train.pred = predict(best.tree, newdata = ames.test)
prp(best.tree,digit=3,tweak=1.2)
par(mar = c(4,4,1,1))
plot(cv.trees$results$cp, cv.trees$results$AvgLoss, type = "l", ylab = "AvgLoss", xlab = "cp") # line plot
Loss <- function(data, lev = NULL, model = NULL, ...) {
c(AvgLoss = mean(data$weights * (data$obs != data$pred)), Accuracy = mean(data$obs == data$pred))
}
par(mar = c(4,4,1,1))
plot(cv.trees$results$cp, cv.trees$results$AvgLoss, type = "l", ylab = "AvgLoss", xlab = "cp") # line plot
printcp(cv.trees)
cv.trees = train(SalePrice ~ ., method = "rpart", data = ames.train,
trControl = trainControl(method = "cv", number = 10), summaryFunction=Loss,
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
cv.trees = train(SalePrice ~ ., method = "rpart", data = ames.train,
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
cv.trees
best.tree <- cv.trees$finalModel
best.tree
par(mar = c(4,4,1,1))
plot(cv.trees$results$cp, cv.trees$results$Rsquared, type = "l", ylab = "AvgLoss", xlab = "cp") # line plot
# prp(best.tree,digit=3,tweak=1.2)
ames.mm <- as.data.frame(model.matrix(SalePrice~., data=ames.test))
cart.train.pred = predict(best.tree, newdata = ames.mm)
# prp(best.tree,digit=3,tweak=1.2)
ames.mm <- as.data.frame(model.matrix(SalePrice~.+0, data=ames.test))
cart.train.pred = predict(best.tree, newdata = ames.mm)
View(best.tree)
pdf('CART.pdf',12,12)
prp(cv.trees$finalModel,varlen=0,faclen=0,digits=3)
dev.off()
rf.cv = train(SalePrice~., data = ames.train, method="rf", ntree=80, nodesize=25,
trControl=trainControl(method="cv", number=5),
tuneGrid=data.frame(mtry=seq(2,30,2)))
best.rf = rf.cv$finalModel
rf.cv
plot(rf.cv$results$mtry, rf.cv$results$Rsquared, type = "l", ylab = "AvgLoss", xlab = "cp") # line plot
plot(rf.cv$results$mtry, rf.cv$results$Rsquared, type = "l", ylab = "RSQ", xlab = "MTRY") # line plot
importance(best.rf)
cv.trees$results$RsquaredSD
cv.trees$results$Rsquared
# prp(best.tree,digit=3,tweak=1.2)
## CANNOT GET THIS PART TO WORK FOR SOME REASON
data.frame.ames = as.data.frame(model.matrix(SalePrice ~ ., data = ames.train))
cart.train.pred = predict(best.tree, newdata = data.frame.ames)
rf.mod.cv = rf.cv$finalModel
# Finally, we can make predictions
pred.train = predict(rf.mod.cv, newdata=ames.train)
pred.test = predict(rf.mod.cv, newdata=amex.test)
pred.test = predict(rf.mod.cv, newdata=ames.test)
#best.rf = rf.cv$finalModel
#rf.cv
plot(rf.cv$results$mtry, rf.cv$results$Rsquared, type = "l", ylab = "RSQ", xlab = "MTRY") # line plot
rf.mod.cv = rf.cv$finalModel
rf.mod.cv
cv.trees.cart = train(SalePrice ~ ., method = "rpart", data = ames.train,
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
pdf('CART.pdf',12,12)
prp(cv.trees.cart$finalModel,varlen=0,faclen=0,digits=3)
dev.off()
cv.trees.cart
# par(mar = c(4,4,1,1))
plot(cv.trees.cart$results$cp, cv.trees.cart$results$Rsquared, type = "l", ylab = "AvgLoss", xlab = "cp") # line plot
best.tree <- cv.trees$finalModel
best.tree <- cv.trees.cart$finalModel
cv.trees.cart$results$Rsquared
# par(mar = c(4,4,1,1))
plot(cv.trees.cart$results$cp, cv.trees.cart$results$Rsquared, type = "l", ylab = "RSQ", xlab = "cp") # line plot
best.tree.cart <- cv.trees.cart$finalModel
# prp(best.tree,digit=3,tweak=1.2)
## CANNOT GET THIS PART TO WORK FOR SOME REASON
data.frame.ames = as.data.frame(model.matrix(SalePrice ~ ., data = ames.train))
ames.mm <- as.data.frame(model.matrix(SalePrice~.+0, data = ames.test))
cart.train.pred = predict(best.tree, newdata = ames.mm)
summary(regressor)
ames.mm
amex.test
ames.test
setwd("~/Library/CloudStorage/OneDrive-MassachusettsInstituteofTechnology/Analytics Edge/Recitation/Rec6")
library(caret)
library(rpart)
library(rpart.plot)
library(caTools)
library(dplyr)
library(randomForest)
# Load the parole data
parole = read.csv("NYCparole.csv")
# Let us take a look at the structure of the data
str(parole)
# We set a seed so that everyone will get the same random split.
set.seed(144)
# As usual, we will set the seed and split our data into training and testing
# We perform stratified sampling to select the same proportion of violators in both datasets
# We had seen this function previously:
#split = createDataPartition(parole$Violator, p = 0.7, list = FALSE)
# Here is another (equivalent) one:
split <- sample.split(parole$Violator, 0.7)
parole.train = parole[split,]
parole.test = parole[!split,]
parole.mod = rpart(Violator~., data = parole.train)
pred = predict(parole.mod, newdata=parole.test, type="class") # 0 or 1
# Like before, we plot using:
prp(parole.mod)
pred = predict(parole.mod, newdata=parole.test, type="class") # 0 or 1
# You might see that Violator shows up as Factor or as int, depending on your settings
# If you see an int, you will have to convert Violator to a factor
# so that CART classification and not regression
# If you see a factor for Violator, you can run the line anyway - it will not have any effect.
parole$Violator = as.factor(parole$Violator)  # Dependent Variable - Classification, not Regression
# We set a seed so that everyone will get the same random split.
set.seed(144)
# As usual, we will set the seed and split our data into training and testing
# We perform stratified sampling to select the same proportion of violators in both datasets
# We had seen this function previously:
#split = createDataPartition(parole$Violator, p = 0.7, list = FALSE)
# Here is another (equivalent) one:
split <- sample.split(parole$Violator, 0.7)
parole.train = parole[split,]
parole.test = parole[!split,]
parole.mod = rpart(Violator~., data = parole.train)
# Like before, we plot using:
prp(parole.mod)
pred = predict(parole.mod, newdata=parole.test, type="class") # 0 or 1
parole.train
# Let us take a look at the structure of the data
str(parole)
ames = read.csv("ames.csv")
# Create a user-defined function (returns a 2-element array)
Loss <- function(data, lev = NULL, model = NULL, ...) {
c(AvgLoss = mean(data$weights * (data$obs != data$pred)), Accuracy = mean(data$obs == data$pred))
}
set.seed(123)
# Create a user-defined function (returns a 2-element array)
Loss <- function(data, lev = NULL, model = NULL, ...) {
c(AvgLoss = mean(data$weights * (data$obs != data$pred)), Accuracy = mean(data$obs == data$pred))
}
# CV on cp vals
cv.trees = train(Violator~Male+Age+TimeServed+Class+Multiple+InCity,  # *explicitly* enumerate the indep. vars
data=parole.train,
method = "rpart",
weights = ifelse(parole.train$Violator == 1, 20, 1), # loss function
trControl = trainControl(method = "cv", number = 10, summaryFunction=Loss), # 10-fold cv
metric="AvgLoss", maximize=FALSE,                    # minimize the AvgLoss
tuneGrid = data.frame(.cp = seq(0, .04, by=.002)))   # cp vals: 0, .002, .004, ..., .04; . is part of the syntax
# OK, that's a lot to take in... so what exactly did we just do? Let's look at it
cv.trees
# The easiest way to extract the model corresponding to the best cp value
# is to go back to the cv.trees object and pull out the part saved as "finalModel"
my.best.tree = cv.trees$finalModel
my.best.tree
# We can plot the tree
prp(my.best.tree,digit=3,tweak=1.2)
# +0 removes the intercept; ?formula
# first convert to a model matrix (mm)
# then convert to a data frame (to interface with predict())
# Using this new dataset, we can apply the usual "predict" function
best.tree.pred = predict(my.best.tree, newdata = parole.mm, type="class")
# +0 removes the intercept; ?formula
# first convert to a model matrix (mm)
# then convert to a data frame (to interface with predict())
# Using this new dataset, we can apply the usual "predict" function
best.tree.pred = predict(my.best.tree, newdata = parole.test, type="class")
parole.test
# We can predict using the tree
# The first line below simply transforms the test set into a "model matrix".
# For instance, instead of having a "Class" variable with values "A", "B", "C", etc.,
# we have a set of categorical binary varibales ("Class A", "Class B", etc.)
parole.mm <- as.data.frame(model.matrix(Violator~.+0, data=parole.test))
parole.mm
setwd("~/AnalyticsEdge/AnalyticsEdge/HW_4")
ames = read.csv("ames.csv")
set.seed(15071)
split = createDataPartition(ames$SalePrice, p = 0.7, list = FALSE)
ames.train = ames[split,]
ames.test = ames[-split,]
head(ames.train)
cv.trees.cart = train(SalePrice ~ ., method = "rpart", data = ames.train,
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
pdf('CART.pdf',12,12)
prp(cv.trees.cart$finalModel,varlen=0,faclen=0,digits=3)
dev.off()
cv.trees.cart
# par(mar = c(4,4,1,1))
plot(cv.trees.cart$results$cp, cv.trees.cart$results$Rsquared, type = "l", ylab = "RSQ", xlab = "cp") # line plot
best.tree.cart <- cv.trees.cart$finalModel
cv.trees.cart$results$Rsquared
prp(best.tree,digit=3,tweak=1.2)
prp(best.tree.cart,digit=3,tweak=1.2)
# prp(best.tree.cart,digit=3,tweak=1.2)
## CANNOT GET THIS PART TO WORK FOR SOME REASON
data.frame.ames = as.data.frame(model.matrix(SalePrice ~ ., data = ames.train))
data.frame.ames
ames.mm <- as.data.frame(model.matrix(SalePrice~.+0, data = data.frame.ames))
ames.mm <- as.data.frame(model.matrix(SalePrice~.+0, data = ames.test))
cart.train.pred = predict(best.tree, newdata = ames.mm)
cart.train.pred = predict(best.tree.cart, newdata = ames.mm)
ames.mm
### Question a.
regressor = lm(SalePrice ~ ., data = ames.train)
summary(regressor)
cart.train.pred = predict(best.tree.cart, newdata = data.frame.ames)
ames.mm <- as.data.frame(model.matrix(SalePrice~., data = ames.test))
ames.mm
cart.train.pred = predict(best.tree.cart, newdata = ames.mm)
ames.mm.allData <- as.data.frame(model.matrix(SalePrice~., data = ames))
ames.mm.allData
ames.test.mm = ames.mm.allData[-split,]
cart.train.pred = predict(best.tree.cart, newdata = ames.test.mm)
ames.mm.allData <- as.data.frame(model.matrix(SalePrice~., data = ames))
ames.test.mm = ames.mm.allData[-split,]
ames.train.mm.initial = ames.mm.allData[split,]
# prp(best.tree.cart,digit=3,tweak=1.2)
## CANNOT GET THIS PART TO WORK FOR SOME REASON
ames.train.mm = as.data.frame(model.matrix(SalePrice ~ ., data = ames.train))
cart.test.pred = predict(best.tree.cart, newdata = ames.test.mm)
cart.train.pred = predict(best.tree.cart, newdata = ames.train.mm)
cart.train.pred.initial = predict(best.tree.cart, newdata = ames.train.mm.initial)
SSTTrain = sum((ames.train$SalePrice - mean(ames.train$SalePrice))^2)
SSETrain = sum((cart.train.pred - ames.train$SalePrice)^2)
R2_CART_treeFinal <- 1 - SSETrain/SSTTrain
SSTTest = sum((ames.test$SalePrice - mean(ames.train$SalePrice))^2)
SSETest = sum((cart.test.pred - ames.test$SalePrice)^2)
OSR2_CART_treeFinal <- 1 - SSETest/SSTTest
R2_CART_treeFinal
OSR2_CART_treeFinal
# par(mar = c(4,4,1,1))
plot(cv.trees.cart$results$cp, cv.trees.cart$results$Rsquared, type = "l", ylab = "RSQ", xlab = "cp") # line plot
SSTTrain
SSETrain
cv.trees.cart$finalModel
cv.trees.cart
OSR2_CART_treeFinal
R2_CART_treeFinal
SSETrain = sum((cart.train.pred.initial - ames.train$SalePrice)^2)
R2_CART_treeFinal <- 1 - SSETrain/SSTTrain
R2_CART_treeFinal
SSTTrain = sum((ames.train$SalePrice - mean(ames.train$SalePrice))^2)
SSETrain = sum((cart.train.pred - ames.train$SalePrice)^2)
R2_CART_treeFinal <- 1 - SSETrain/SSTTrain
R2_CART_treeFinal
rf.cv = train(SalePrice~., data = ames.train, method="rf", ntree=80, nodesize=25,
trControl=trainControl(method="cv", number=5),
tuneGrid=data.frame(mtry=seq(2,30,2)))
rf.mtry = train(SalePrice~., data = ames.train, method="rf", ntree=80, nodesize=25,
trControl=trainControl(method="cv", number=5),
tuneGrid=data.frame(mtry=seq(2,30,2)))
#best.rf = rf.cv$finalModel
#rf.cv
plot(rf.mtry$results$mtry, rf.cv$results$Rsquared, type = "l", ylab = "RSQ", xlab = "MTRY") # line plot
best.rf = rf.mtry$finalModel
rf.mod.mtry = rf.mtry$finalModel
# Finally, we can make predictions
pred.train = predict(rf.mod.mtry, newdata=ames.train.mm)
pred.test = predict(rf.mod.mtry, newdata=ames.test.mm)
# Finally, we can make predictions
rf.pred.train = predict(rf.mod.mtry, newdata=ames.train.mm)
rf.pred.test = predict(rf.mod.mtry, newdata=ames.test.mm)
SSTTrain = sum((ames.train$SalePrice - mean(ames.train$SalePrice))^2)
SSETrain = sum((rf.pred.train - ames.train$SalePrice)^2)
R2_CART_treeFinal <- 1 - SSETrain/SSTTrain
SSTTest = sum((ames.test$SalePrice - mean(ames.train$SalePrice))^2)
SSETest = sum((rf.pred.test - ames.test$SalePrice)^2)
OSR2_CART_treeFinal <- 1 - SSETest/SSTTest
R2_CART_treeFinal
OSR2_CART_treeFinal
MAE = MAE(rf.pred.train, ames.train$SalePrice)
MAE
lin.reg.R2.train <-
lin.reg.MAE.train <-
lin.reg.RMSE.train <-
lin.reg.R2.test <-
lin.reg.MAE.test <-
lin.reg.RMSE.test <-
CART.R2.train <-
CART.MAE.train <- MAE(cart.train.pred, ames.train$SalePrice)
CART.MAE.train <- MAE(cart.train.pred, ames.train$SalePrice)
CART.MAE.train
reg.test.pred = predict(regressor, newdata = ames.test.mm)
ames.test.mm = ames.mm.allData[-split,]
ames.train.mm = ames.mm.allData[split,]
### Question a.
regressor = lm(SalePrice ~ ., data = ames.train)
summary(regressor)
reg.test.pred = predict(regressor, newdata = ames.test.mm)
reg.test.pred = predict(regressor, newdata = ames.test)
reg.train.pred = predict(regressor, newdata = ames.train)
SSTTrain = sum((ames.train$SalePrice - mean(ames.train$SalePrice))^2)
SSTTest = sum((ames.test$SalePrice - mean(ames.train$SalePrice))^2)
reg.SSETrain = sum((reg.train.pred - ames.train$SalePrice)^2)
reg.R2_CART_treeFinal <- 1 - reg.SSETrain/SSTTrain
reg.SSETest = sum((cart.test.pred - ames.test$SalePrice)^2)
reg.OSR2_CART_treeFinal <- 1 - reg.SSETest/SSTTest
reg.R2_CART_treeFinal
reg.OSR2_CART_treeFinal
ames = read.csv("ames.csv")
set.seed(15071)
split = createDataPartition(ames$SalePrice, p = 0.7, list = FALSE)
ames.train = ames[split,]
ames.test = ames[-split,]
head(ames.train)
ames.mm.allData <- as.data.frame(model.matrix(SalePrice~., data = ames))
ames.test.mm = ames.mm.allData[-split,]
ames.train.mm = ames.mm.allData[split,]
SSTTrain = sum((ames.train$SalePrice - mean(ames.train$SalePrice))^2)
SSTTest = sum((ames.test$SalePrice - mean(ames.train$SalePrice))^2)
### Question a.
regressor = lm(SalePrice ~ ., data = ames.train)
reg.test.pred = predict(regressor, newdata = ames.test)
reg.train.pred = predict(regressor, newdata = ames.train)
reg.SSETrain = sum((reg.train.pred - ames.train$SalePrice)^2)
reg.R2 <- 1 - reg.SSETrain/SSTTrain
reg.SSETest = sum((cart.test.pred - ames.test$SalePrice)^2)
reg.SSETest = sum((reg.test.pred - ames.test$SalePrice)^2)
reg.OSR2 <- 1 - reg.SSETest/SSTTest
reg.OSR2
reg.R2
cv.trees.cart = train(SalePrice ~ ., method = "rpart", data = ames.train,
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(.cp = seq(.00002,.002,.00002)))
best.tree.cart <- cv.trees.cart$finalModel
cart.test.pred = predict(best.tree.cart, newdata = ames.test.mm)
cart.train.pred = predict(best.tree.cart, newdata = ames.train.mm)
cart.SSETrain = sum((cart.train.pred - ames.train$SalePrice)^2)
R2_CART_treeFinal <- 1 - SSETrain/SSTTrain
R2_CART_treeFinal <- 1 - cart.SSETrain/SSTTrain
cart.SSETest = sum((cart.test.pred - ames.test$SalePrice)^2)
OSR2_CART_treeFinal <- 1 - cart.SSETest/SSTTest
R2_CART_treeFinal
OSR2_CART_treeFinal
rf.mtry = train(SalePrice~., data = ames.train, method="rf", ntree=80, nodesize=25,
trControl=trainControl(method="cv", number=5),
tuneGrid=data.frame(mtry=seq(2,30,2)))
rf.mod.mtry = rf.mtry$finalModel
# Finally, we can make predictions
rf.pred.train = predict(rf.mod.mtry, newdata=ames.train.mm)
rf.pred.test = predict(rf.mod.mtry, newdata=ames.test.mm)
rf.SSETrain = sum((rf.pred.train - ames.train$SalePrice)^2)
R2_CART_treeFinal <- 1 - rf.SSETrain/SSTTrain
rf.SSETest = sum((rf.pred.test - ames.test$SalePrice)^2)
OSR2_CART_treeFinal <- 1 - rf.SSETest/SSTTest
R2_RF_treeFinal <- 1 - rf.SSETrain/SSTTrain
rf.SSETest = sum((rf.pred.test - ames.test$SalePrice)^2)
OSR2_RF_treeFinal <- 1 - rf.SSETest/SSTTest
R2_CART_treeFinal <- 1 - cart.SSETrain/SSTTrain
cart.SSETest = sum((cart.test.pred - ames.test$SalePrice)^2)
OSR2_CART_treeFinal <- 1 - cart.SSETest/SSTTest
lin.reg.R2.train <- reg.R2
lin.reg.MAE.train <- MAE(reg.train.pred, ames.train$SalePrice)
lin.reg.R2.test <- reg.OSR2
lin.reg.MAE.test <- MAE(reg.test.pred, ames.train$SalePrice)
lin.reg.R2.test <- reg.OSR2
lin.reg.MAE.test <- MAE(reg.test.pred, ames.test$SalePrice)
CART.R2.train <- R2_CART_treeFinal
CART.MAE.train <- MAE(cart.train.pred, ames.train$SalePrice)
CART.R2.test <- OSR2_CART_treeFinal
CART.MAE.test <- MAE(cart.test.pred, ames.test$SalePrice)
RF.R2.train <- R2_RF_treeFinal
RF.MAE.train <- MAE(rf.pred.train, ames.train$SalePrice)
RF.R2.test <- OSR2_RF_treeFinal
RF.MAE.test <- MAE(rf.pred.test, ames.test$SalePrice)
### Question d.
sqrt(mean((ames.test$SalePrice - reg.train.pred) ^2))
### Question d.
sqrt(mean((ames.test$SalePrice - reg.test.pred) ^2))
lin.reg.RMSE.train <- sqrt(mean((ames.train$SalePrice - reg.train.pred) ^2))
lin.reg.RMSE.train
lin.reg.R2.train <- reg.R2
lin.reg.MAE.train <- MAE(reg.train.pred, ames.train$SalePrice)
lin.reg.RMSE.train <- sqrt(mean((ames.train$SalePrice - reg.train.pred) ^2))
lin.reg.R2.test <- reg.OSR2
lin.reg.MAE.test <- MAE(reg.test.pred, ames.test$SalePrice)
lin.reg.RMSE.test <- sqrt(mean((ames.test$SalePrice - reg.test.pred) ^2))
CART.R2.train <- R2_CART_treeFinal
CART.MAE.train <- MAE(cart.train.pred, ames.train$SalePrice)
CART.RMSE.train <- sqrt(mean((ames.train$SalePrice - cart.train.pred) ^2))
CART.R2.test <- OSR2_CART_treeFinal
CART.MAE.test <- MAE(cart.test.pred, ames.test$SalePrice)
CART.RMSE.test <- sqrt(mean((ames.test$SalePrice - cart.test.pred) ^2))
RF.R2.train <- R2_RF_treeFinal
RF.MAE.train <- MAE(rf.pred.train, ames.train$SalePrice)
RF.RMSE.train <- sqrt(mean((ames.train$SalePrice - rf.train.pred) ^2))
RF.R2.train <- R2_RF_treeFinal
RF.MAE.train <- MAE(rf.pred.train, ames.train$SalePrice)
RF.RMSE.train <- sqrt(mean((ames.train$SalePrice - rf.pred.train) ^2))
RF.R2.test <- OSR2_RF_treeFinal
RF.MAE.test <- MAE(rf.pred.test, ames.test$SalePrice)
RF.RMSE.test <- sqrt(mean((ames.test$SalePrice - rf.pred.test) ^2))
### Question d.
lin.reg.R2.train <- reg.R2
lin.reg.MAE.train <- MAE(reg.train.pred, ames.train$SalePrice)
lin.reg.RMSE.train <- sqrt(mean((ames.train$SalePrice - reg.train.pred) ^2))
lin.reg.R2.test <- reg.OSR2
lin.reg.MAE.test <- MAE(reg.test.pred, ames.test$SalePrice)
lin.reg.RMSE.test <- sqrt(mean((ames.test$SalePrice - reg.test.pred) ^2))
CART.R2.train <- R2_CART_treeFinal
CART.MAE.train <- MAE(cart.train.pred, ames.train$SalePrice)
CART.RMSE.train <- sqrt(mean((ames.train$SalePrice - cart.train.pred) ^2))
CART.R2.test <- OSR2_CART_treeFinal
CART.MAE.test <- MAE(cart.test.pred, ames.test$SalePrice)
CART.RMSE.test <- sqrt(mean((ames.test$SalePrice - cart.test.pred) ^2))
RF.R2.train <- R2_RF_treeFinal
RF.MAE.train <- MAE(rf.pred.train, ames.train$SalePrice)
RF.RMSE.train <- sqrt(mean((ames.train$SalePrice - rf.pred.train) ^2))
RF.R2.test <- OSR2_RF_treeFinal
RF.MAE.test <- MAE(rf.pred.test, ames.test$SalePrice)
RF.RMSE.test <- sqrt(mean((ames.test$SalePrice - rf.pred.test) ^2))
# Summary
summary_statistics <- data.frame(
IS.R2 = c(lin.reg.R2.train,CART.R2.train,RF.R2.train),
IS.MAE = c(lin.reg.MAE.train,CART.MAE.train,RF.MAE.train),
IS.RMSE = c(lin.reg.RMSE.train,CART.RMSE.train,RF.RMSE.train),
OOS.R2 = c(lin.reg.R2.test,CART.R2.test,RF.R2.test),
OOS.MAE = c(lin.reg.MAE.test,CART.MAE.test,RF.MAE.test),
OOS.RMSE = c(lin.reg.RMSE.test,CART.RMSE.test,RF.RMSE.test)
)
summary_statistics
sqrt(mean((ames.test$SalePrice - rf.pred.test) ^2))
RMSE(rf.pred.test, ames.test$SalePrice)
summary(regressor)
plot(regressor)
library(gdata)
ebay.df <- read.xls("eBayAuctions.xls")
table(ebay.df$Competitive.)
ebay.df$competitive.factor <- as.factor(ebay.df$Competitive.)
### (a)
## adaboost
library(adabag)
