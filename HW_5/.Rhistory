setwd("~/AnalyticsEdge/AnalyticsEdge/HW_5")
library(caret)
library(rpart)
library(rpart.plot)
library(caTools)
library(dplyr)
library(randomForest)
library(gdata)
library(ggplot2)
library(tidyr)
# Part (a) code block
# ----------------------------------------------------------------------------------------
data = read.csv("returns.csv")
returns = data[c(2,25:60)]
full_returns = data[,3:122]
agg_data  = aggregate(. ~ Industry, returns, mean)
agg_data
#Start here.
set.seed(2407)
# The kmeans function creates the clusters
# we can set an upper bound to the number of iterations
# of the algorithm
# here we set k=7
km_returns <- kmeans(full_returns, centers = 8, iter.max=100) # centers randomly selected from rows of airline.scaled
class(km_returns) # class: kmeans
names(km_returns) # Take a look at what's inside this object.
# Let's explore the results!
# cluster centroids. Store this result
km_returns.centroids <- km_returns$centers
km_returns.centroids
# cluster for each point. Store the assignment of each point to its corresponding cluster.
km_returns.clusters <- km_returns$cluster
km_returns.clusters
# the sum of the squared distances of each observation from its cluster centroid.
# we use it the measure cluster dissimilarity
km_returns$tot.withinss  # cluster dissimilarity
# the number of observations in each cluster -- table(km$cluster) also works. Store this resul
km_returns.size <- km_returns$size
km_returns.size
# Scree plot for k-means
# For k means, we literally try many value of k and look at their dissimilarity.
# here we test all k from 1 to 100
k_ret.data <- data.frame(k_ret = 1:100)
View(data)
k_ret.data$SS <- sapply(k_ret.data$k_ret, function(k_ret) {
kmeans(full_returns, iter.max=100, k_ret)$tot.withinss
})
pdf('kmeans.pdf',8,5)
plot(k_ret.data$k_ret, k_ret.data$SS, type="l")
dev.off()
# Compare the clusters from kmeans and Hierarchical. Do some clusters "match up"? (Yes, because there are alot of zeros.)
table(h.clusters, km.clusters)
d <- dist(full_returns, method="euclidean")    # method = "euclidean"
class(d)
# Creates the Hierarchical clustering
hclust.mod <- hclust(d, method="ward.D2")
# Plot the hierarchy structure (dendrogram)
pdf('dendrogram.pdf',8,5)
plot(hclust.mod, labels=F, ylab="Dissimilarity", xlab = "", sub = "")
dev.off()
# To choose a good value for k, we need to create the scree plot: dissimilarity for each k
# the next line puts this data in the right form to b?mixtoolse plotted
hc.dissim <- data.frame(k = seq_along(hclust.mod$height),   # index: 1,2,...,length(hclust.mod$height)
dissimilarity = rev(hclust.mod$height)) # reverse elements
(hc.dissim)
# Improvement in dissimilarity for increasing number of clusters
hc.dissim.dif = head(hc.dissim,-1)-tail(hc.dissim,-1)  # all but the last of hc.dissim - all but the first, basically a shifted difference
head(hc.dissim.dif,10)
# now that we have k (we chose k=7 in the lecture), we can construct the clusters
h.clusters <- cutree(hclust.mod, 8)
h.clusters
# The *centroid* for a cluster is the mean value of all points in the cluster:
aggregate(full_returns, by=list(h.clusters), mean) # Compute centroids
# Number of companies in each cluster
table(h.clusters)
# Number of companies per industry in each cluster
table(data$Industry, h.clusters)
# Average returns by cluster for Oct 2008
aggregate(full_returns$avg200810, by=list(h.clusters), mean)
# Average returns by cluster for March 2009
aggregate(full_returns$avg200903, by=list(h.clusters), mean)
# Compare the clusters from kmeans and Hierarchical. Do some clusters "match up"? (Yes, because there are alot of zeros.)
table(h.clusters, km.clusters)
# Compare the clusters from kmeans and Hierarchical. Do some clusters "match up"? (Yes, because there are alot of zeros.)
table(h.clusters, km_returns.centroids)
# Compare the clusters from kmeans and Hierarchical. Do some clusters "match up"? (Yes, because there are alot of zeros.)
table(h.clusters, km_returns.clusters)
km_returns.clusters
km_returns.centroids
# Average returns by cluster for Oct 2008
aggregate(full_returns$avg200810, by=list(km_returns.clusters), mean)
# Average returns by cluster for March 2009
aggregate(full_returns$avg200903, by=list(km_returns.clusters), mean)
# Average returns by cluster for March 2009
aggregate(full_returns$avg200903, by=list(h.clusters), mean)
# Compare the clusters from kmeans and Hierarchical. Do some clusters "match up"? (Yes, because there are alot of zeros.)
table(h.clusters, km_returns.clusters)
# ----------------------------------------------------------------------------------------
# Part e.
# ----------------------------------------------------------------------------------------
?silhouette
library("gmp")
library("ClusterR")
# ----------------------------------------------------------------------------------------
# Part e.
# ----------------------------------------------------------------------------------------
?silhouette
# Clustering
library(cluster)
?silhouette
silhouette(km_returns.clusters,dist(full_returns))
mean(ss[, 3])
ss = silhouette(km_returns.clusters,dist(full_returns))
mean(ss[, 3])
k_means_ss = silhouette(km_returns.clusters,dist(full_returns))
mean(k_means_ss[, 3])
hier_ss = silhouette(h.clusters,dist(full_returns))
mean(hier_ss[, 3])
plot(km_returns.clusters, col=1:8, border=NA)
plot(k_means_ss, col=1:8, border=NA)
k_means_ss
mean(k_means_ss[cluster, 3])
aggregate(k_means_ss, by=list(km_returns.clusters), mean)
plot(hier_ss, col=1:8, border=NA)
hier_ss = silhouette(h.clusters,dist(full_returns))
aggregate(hier_ss, by=list(h.clusters), mean)
mean(hier_ss[, 3])
mean(k_means_ss[, 3])
k_means_ss = silhouette(km_returns.clusters,dist(full_returns))
mean(k_means_ss[, 3])
aggregate(k_means_ss, by=list(km_returns.clusters), mean)
hier_ss = silhouette(h.clusters,dist(full_returns))
aggregate(hier_ss, by=list(h.clusters), mean)
mean(hier_ss[, 3])
?silhouette
hier_ss
source("~/AnalyticsEdge/AnalyticsEdge/HW_5/PSet5.R")
